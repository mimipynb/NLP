{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetining DistilRoberta MLM with Emotion dialogue \n",
    "\n",
    "Dated: 30.04.2024\n",
    "\n",
    "Contains full script in fine-tuning **Distil Roberta Mask Language Model** on emotion dialogue datasets. \n",
    "\n",
    "Method description:\n",
    "- Special masks: Added special masks to the language model that maps to emotion labels.\n",
    "- Data transformation process: the dataset was presented to the language model as pair of utterances where the emotion tokens were masked with the special mask token. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3436906bac0347b9ac41304d69eb36ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0febcee62b44868848cc20601fb17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b69d93548c46f8b641567550a96d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model card id set to: darthfalka/emoDialog-distilroberta-base\n",
      "Saving to hub as emoDialog-distilroberta-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.266738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.142731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>0.225085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.155241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.187337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818b00ecdc884f96b7dc632eec4f0700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Fine-tuning MLM to basic with specific prompts.\n",
    "Contains:\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    set_seed,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "dataset_id = 'li2017dailydialog/daily_dialog'\n",
    "model_id = \"distilbert/distilroberta-base\"\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# load tokenizer and model to fine-tune\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id, output_attentions=True)\n",
    "\n",
    "# setup default datacollator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.20, return_tensors=\"pt\")\n",
    "\n",
    "classification_mapping = {\n",
    "    \"act\": {\n",
    "        0: \"unknown\",\n",
    "        1: \"inform\",\n",
    "        2: \"question\",\n",
    "        3: \"directive\",\n",
    "        4: \"commissive\"\n",
    "    },\n",
    "    \"emotion\": {\n",
    "        0: \"neutral\",\n",
    "        1: \"anger\",\n",
    "        2: \"disgust\",\n",
    "        3: \"fear\",\n",
    "        4: \"happiness\",\n",
    "        5: \"sadness\",\n",
    "        6: \"surprise\"\n",
    "    }\n",
    "}\n",
    "\n",
    "DIALOG_PROMPT = \"\"\"User: [Emotion: {emotion_label}] [Action: {action_label}] {dialogue_input} \\nAgent: [Emotion: {respond_emotion_label}] [Action: {respond_emotion_label}] {dialogue_response}\"\"\"\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    query = DIALOG_PROMPT.format(\n",
    "        emotion_label=data[\"emotion\"],\n",
    "        action_label=data[\"act\"],\n",
    "        dialogue_input=data[\"dialog\"],\n",
    "        respond_emotion_label=data[\"response_emote\"],\n",
    "        respond_action_label=data[\"act\"],\n",
    "        dialogue_response=data[\"response\"]\n",
    "    )\n",
    "    # Tokenize the prompt\n",
    "    tokenized = tokenizer(query, truncation=True, padding=\"max_length\", max_length=256, return_special_tokens_mask=False, return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def cleaner(df):\n",
    "    \"\"\"Cleans the dataset to be put into prompt\"\"\"\n",
    "\n",
    "    df['dial_id'] = df.index.values\n",
    "    df = df.explode(['dialog', 'act', 'emotion'], ignore_index=True)\n",
    "    df[\"act\"] = df[\"act\"].map(classification_mapping[\"act\"])\n",
    "    df[\"emotion\"] = df[\"emotion\"].map(classification_mapping[\"emotion\"])\n",
    "    df['response'] = df.groupby('dial_id')['dialog'].shift(-1)\n",
    "    df['response_emote'] = df.groupby('dial_id')['emotion'].shift(-1)\n",
    "    df[\"response\"].fillna(\". \"+ tokenizer.pad_token, inplace=True)\n",
    "    df[\"response_emote\"].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "data = load_dataset(dataset_id)\n",
    "train_data = cleaner(data[\"train\"].to_pandas())\n",
    "valid_data = cleaner(data[\"validation\"].to_pandas())\n",
    "test_data = cleaner(data[\"test\"].to_pandas())\n",
    "\n",
    "train_df = Dataset.from_pandas(train_data)\n",
    "valid_df = Dataset.from_pandas(valid_data)\n",
    "test_df = Dataset.from_pandas(test_data)\n",
    "\n",
    "train_dfs = train_df.map(preprocess_dataset, batched=True, remove_columns=train_df.column_names)\n",
    "valid_dfs = valid_df.map(preprocess_dataset, batched=True, remove_columns=valid_df.column_names)\n",
    "test_dfs = test_df.map(preprocess_dataset, batched=True, remove_columns=test_df.column_names)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# defining save / load / HF resp paths\n",
    "new_model_name = \"emoDialog-distilroberta-base\"\n",
    "hub_model_id = f\"darthfalka/{new_model_name}\"\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d%m%Y-%H:%M:%S\")\n",
    "default_path = f\"./modelHistory/{current_date}\"\n",
    "output_path = f\"{default_path}/results\"\n",
    "logs_path = f\"{default_path}/logs\"\n",
    "model_local_path = f\"{default_path}/model\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # directory + hub config\n",
    "    output_dir=output_path,\n",
    "    logging_dir=logs_path,\n",
    "    hub_token=os.getenv(\"HF_KEY\"),\n",
    "    hub_model_id=hub_model_id,\n",
    "    push_to_hub=True,\n",
    "    # local / save defaults e.g. how it was saved\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    prediction_loss_only=False,\n",
    "    # conditioning statement for selecting model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # batch params\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    # training params\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-05,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dfs,\n",
    "    eval_dataset=valid_dfs,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # model_init = model ~  only set if you want to create a new instance for every training loop\n",
    "    # callbacks\n",
    "    # preprocess_logits_for_metrics\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f'Model card id set to: {trainer.hub_model_id}')\n",
    "print(f\"Saving to hub as {new_model_name}\")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(model_local_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

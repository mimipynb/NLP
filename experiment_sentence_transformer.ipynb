{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook of things dealing with Sentence Transformer library\n",
    "\n",
    "Trainers and Evaluation with Sentence Transformers Library on sentence embeddings. \n",
    "\n",
    "Contains\n",
    "- Data Preparation tools according to the loss functions \n",
    "- Available Samplers as part of Data Preprocessing operations in this python library \n",
    "\n",
    "\n",
    "Resources:\n",
    "- https://sbert.net/docs/package_reference/sentence_transformer/sampler.html\n",
    "- https://huggingface.co/blog/train-sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator, EmbeddingSimilarityEvaluator\n",
    "\n",
    "model_card = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "data_path = \"/Users/mimiphan/Projects/wsdm-cup-multilingual-chatbot-arena/train.parquet\"\n",
    "\n",
    "def new_experiment(model_card=model_card, num_samples=2000):\n",
    "    model = SentenceTransformer(model_card)\n",
    "    ds = pd.read_parquet(data_path).sample(num_samples) \n",
    "    ds.reset_index(drop=True, inplace=True)\n",
    "    return model, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Target, Anchor and Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Column labels** \n",
    "1. `sentence` \n",
    "2. `label` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Column labels**\n",
    "\n",
    "1. texts: 2 texts labeled 'sentence1' and 'sentence2'\n",
    "    - `sentence1`\n",
    "    - `sentence2`\n",
    "\n",
    "2. labels: either 0 or 1 where 0 indicates increasing the distance between the 2 embeddings\n",
    "    - `labels` \n",
    "    \n",
    "**Evaluation Tools**\n",
    "- Embedding Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, ds = new_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence gives the winning text responses only \n",
    "ds['sentence1'] = [i['prompt'] + i['response_a'] if i['winner'] == 'model_a' else i['prompt'] + i['response_b'] for _, i in ds.iterrows()] # winning data rows\n",
    "ds['sentence2'] = [i['prompt'] + i['response_b'] if i['winner'] == 'model_a' else i['prompt'] + i['response_a'] for _, i in ds.iterrows()] # losing data rows\n",
    "ds['label'] = np.zeros(ds.shape[0], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.ContrastiveLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_column = ['sentence1', 'sentence2', 'label']\n",
    "data = Dataset.from_pandas(ds[task_column]).train_test_split(train_size=0.7, shuffle=True)\n",
    "train_data, eval_data = data['train'], data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label'],\n",
       "    num_rows: 1400\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model, \n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=eval_data, \n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmimipynb\u001b[0m (\u001b[33mdeathstar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/march/lib/python3.12/site-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mimiphan/Projects/wandb/run-20250122_062041-vpettngs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deathstar/sentence-transformers/runs/vpettngs' target=\"_blank\">tmp_trainer</a></strong> to <a href='https://wandb.ai/deathstar/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deathstar/sentence-transformers' target=\"_blank\">https://wandb.ai/deathstar/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deathstar/sentence-transformers/runs/vpettngs' target=\"_blank\">https://wandb.ai/deathstar/sentence-transformers/runs/vpettngs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 06:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1502fa31e647c98a2d13521b35647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=525, training_loss=0.0022897982316291226, metrics={'train_runtime': 388.6186, 'train_samples_per_second': 10.808, 'train_steps_per_second': 1.351, 'total_flos': 0.0, 'train_loss': 0.0022897982316291226, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.034214526414871216,\n",
       " 'eval_runtime': 12.3156,\n",
       " 'eval_samples_per_second': 48.719,\n",
       " 'eval_steps_per_second': 6.09,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=None, label_ids=None, metrics={'test_loss': 0.030817203223705292, 'test_runtime': 28.7547, 'test_samples_per_second': 48.688, 'test_steps_per_second': 6.086})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_eval = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=eval_data['sentence1'], \n",
    "    sentences2=eval_data['sentence2'], \n",
    "    scores=eval_data['label'], \n",
    "    name='embedding_evaluation',\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b5d7c9a825473d8bff840107fd24d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2f0b2d554e4619a8564fdfded7626c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/march/lib/python3.12/site-packages/sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:206: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  eval_pearson, _ = pearsonr(labels, scores)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/march/lib/python3.12/site-packages/sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py:207: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  eval_spearman, _ = spearmanr(labels, scores)\n"
     ]
    }
   ],
   "source": [
    "results = embedding_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_mse = MSEEvaluator(\n",
    "    source_sentences=eval_data[\"english\"],\n",
    "    target_sentences=eval_data[\"non_english\"],\n",
    "    name=\"en-fr-dev\",\n",
    "    teacher_model=model,\n",
    "    batch_size=32,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
